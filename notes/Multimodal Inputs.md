---
title: "Multimodal Inputs"
tags: ["concept", "input-modality", "ocr", "images", "voice"]
date: 2026-01-06
status: draft
---

# Multimodal Inputs

Software development is evolving beyond the keyboard. **Multimodal Inputs** refer to the diverse range of methods developers can now use to provide context and instructions to AI coding agents.

## Modalities

1.  **Text**: The traditional standard. Precise, structured, but sometimes slow.
2.  **[[Voice Typing]]**: High-bandwidth, natural language input. Great for intent, context, and speed.
3.  **Images / OCR**:
    *   **Napkin Diagrams**: Sketching logic or UI on paper and snapping a photo.
    *   **Screenshots**: Capturing UI bugs or reference designs.
    *   **Whiteboards**: Photos of architectural planning sessions.

## The AI Unifier

In this paradigm, the AI acts as the **universal translator**. It ingests these different modalities—transcribing voice, performing OCR on images, interpreting sketches—and converts them all into the common language of software engineering: **Code**, **Specifications**, and **Commit Messages**.

## Implications

*   **Accessibility**: coding becomes more accessible to those who may struggle with typing or prefer other communication styles.
*   **Richness**: Some concepts are easier to draw (topology) or say (intent) than to type (syntax). Multimodal inputs allow the developer to choose the "highest bandwidth" channel for the specific information they are conveying.

## References

*   [Thread by @levelsio](https://x.com/levelsio/status/2008316983306027254): A practical example of shifting high-level software engineering tasks to a mobile device via a combination of voice (STT) and terminal agents.
