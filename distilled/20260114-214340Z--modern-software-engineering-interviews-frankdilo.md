---
title: "Modern Software Engineering Interviews in the AI Era"
date: 2026-01-14
source: "[[2026-01-14T234322+0200 - Thread by @frankdilo]]"
author:
  - "@frankdilo"
tags:
  - "hiring"
  - "ai-assisted-coding"
  - "agentic-coding"
  - "interviews"
  - "distilled"
---

# Modern Software Engineering Interviews in the AI Era

A comprehensive discussion initiated by @frankdilo on how software engineering interviews must evolve as AI and coding agents make traditional methods (take-homes, live coding) obsolete.

## Summary
The software engineering hiring landscape has shifted dramatically in the last 1.5 years. With AI capable of solving traditional take-home assignments and live coding exercises providing low signal, forward-looking companies are moving toward work trials, collaborative "vibe coding" sessions, and testing for judgment, product sense, and AI orchestration. The core tension is that "execution is cheap, but judgment in context is not."

## Key Points
- **Traditional Methods are "Cooked"**: Take-homes are now seen as "AI slop" and live coding often fails to reveal real-world performance.
- **The Shift to Judgment**: Focus is moving from syntax and manual execution to higher-level decision-making, architectural taste, and the ability to spot bad AI output.
- **Work Trials as Gold Standard**: Paid trials (2 days to 2 weeks) on real codebases are considered the highest-signal methods, though they scale poorly for large candidate pools.
- **Orchestration Skills**: Companies now seek "tech lead-minded" developers who can orchestrate coding agents effectively.
- **Live Collaborative Sessions**: Peer programming with full access to AI tools allows interviewers to see how candidates think, prompt, and review code.
- **Systems Thinking over Coding**: Testing how a candidate approaches a problem and understands the "why" behind product features.

## Concepts and Patterns

### Concepts
- **Agentic Coding**: The baseline skill of using AI agents to ship software efficiently.
- **Judgment over Syntax**: The prioritization of decision quality and code review over raw coding speed.
- **Product-Minded Engineering**: Understanding the business value and "why" behind features rather than just technical implementation.
- **AI Competency Testing**: Evaluating how someone handles AI hallucinations, reviews generated PRBs, and sets up guardrails.

### Patterns
- **Paid Project Trials**: Hiring candidates for a limited-scope real task before a long-term offer.
- **Agentic Coding Round**: A live session where the candidate is expected to ship a feature in ~1 hour using any tools (including agents).
- **Asynchronous Decision Extraction**: Attempting to extract decision quality directly from real code asynchronously (mentioned by @TikZSZ).
- **Background Deep-Dives**: Verifying solid career foundations built before the availability of advanced AI assistants like Claude Code.

## Entities
- @frankdilo: Discussion initiator, looking to hire in 2026.
- @haltakov: Proponent of real tasks in the codebase for part-time trials.
- @manuel_frigerio: Hires for agency and ownership rather than faked expertise.
- @giuseppegurgone: Advocates for building with coding agents to find tech lead-minded folks.
- @dotpem: Notes that Claude Code usage can be spotted and interrogated in interviews.
- **Bridge Jobs**: A startup (bridge-jobs.com) mentioned as solving this by turning JDs into practical projects.

## Notable Quotes
- > "Execution is cheap now, but judgment in real context isn’t." — @TikZSZ
- > "At this point most coding interviews should involve building something with a coding agent." — @giuseppegurgone
- > "Architecture > prompting." — @AbdMuizAdeyemo
- > "Take-homes are AI slop now. Live coding proves nothing. You need to test if they have taste, not syntax." — @AbdMuizAdeyemo
- > "What matters now is judgment, not memorization." — @tamecalm
