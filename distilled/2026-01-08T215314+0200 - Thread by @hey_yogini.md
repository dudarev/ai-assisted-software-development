---
title: "The AI Confidence Gap: Balancing Empowerment and Uncertainty"
source_url: "https://x.com/hey_yogini/status/2009259026438344790"
captured_at: "2026-01-08"
distilled_at: "2026-01-08T19:53:30Z"
raw_refs:
  - "[[raw/2026-01-08T215314+0200 - Thread by @hey_yogini.md]]"
capture_type: "thread"
status: draft
agent: antigravity
model: gemini-3-flash
confidence_notes: "None"
tags: ["vibe-coding", "developer-experience", "ai-assisted-coding", "trust", "maintenance"]
---

## Summary

This thread explores the psychological and professional "confidence gap" that emerges when developers shift from manual coding to using AI agents like Claude Code and Antigravity. While these tools significantly empower developers and increase output speed, many users report a lingering sense of uncertainty about the code they ship. The discussion highlights a fundamental shift in the developer role: from a "maker" who builds mental models through the act of writing, to a "checker" who must verify outputs they haven't "lived through." The consensus suggests that confidence must now be regained through deeper upfront planning, rigorous testing, and smaller, more reviewable PRs.

## Key points

- **Loss of the "Living Through" Experience**: Manually writing code handles edge cases and builds a mental model naturally; AI skips these steps, leaving the developer with a finished product they don't fully "own" mentally.
- **Verification-Based Confidence**: Confidence is shifting from the act of authorship to the rigor of validation (tests, upfront planning, and post-implementation checks).
- **Skill Atrophy vs. Performance Gains**: The performance boost is real, but it comes with the risk of skill atrophy and a decrease in deep system understanding.
- **The "Fix This" Trap**: The speed of AI can encourage a superficial "fix and push" workflow, bypassing the necessary depth of validation.
- **Managing Trust via Granularity**: Trust is easier to maintain with small, incremental AI-assisted PRs rather than large, opaque blobs of code.

## Concepts / principles

**The Mental Model Gap**
When a developer writes every line, they build a robust mental model of the system's execution path. AI generates the end state directly, skipping the "reasoning steps" that help a human understand *why* the code works (or where it might fail).

**Authorship vs. Reviewership**
<!-- TODO: This is related to that idea of shift from executor to orchestrator, though framed from a different angle. and maybe these psychological issues can also be connected to that note as well. -->
The developer is transitioning from a "maker" to a "checker." This shift requires a different set of skills—primarily the ability to rigorously audit code and tests rather than just writing logic.

**Confidence Displacement**
Confidence no longer comes from the muscle memory of writing code. It must be displaced to higher-level activities: better specs, more comprehensive test suites, and runtime telemetry.

## Patterns

**Small PRs for AI Code**
Breaking down AI tasks into smaller, reviewable units to minimize the "blind spots" in generated code.

**Rigorous Test Verification**
Instead of just asking AI to "write tests," developers must manually and rigorously verify that the tests themselves cover the intended behavior and edge cases.

**Telemetry-Based Insight**
Using runtime telemetry and architecture visualization to regain the insight lost when manual implementation is skipped.

## Entities

- **Claude Code**: A primary AI tool mentioned as empowering yet contributing to the confidence gap.
- **Antigravity**: Specifically noted for its rapid impact on developer self-confidence (released ~1.5 months prior to the thread).
- **CodeRabbit**: Suggested as a tool to help stop "slop" from reaching production through AI-assisted review.
- **Gergely Orosz**: Referenced regarding a "brilliant article" on this topic.

## Quotes

> I miss the confidence I used to have on the code and on what I am shipping. No matter how carefully I read every line.
> 
> — @hey_yogini

> Now you're shipping things you've verified but haven't quite lived through. The doubt isn't a bug.
> 
> — @nayanmanihazra

> What’s changed for me is where the confidence comes from—less from writing every line, more from better upfront planning and more thorough testing after. Implementation got faster, thinking + validation got deeper.
> 
> — @Jagadhis02

> When you write code, you build a mental model of why it works. With AI you are reviewing output w/o seeing the steps that led there.
> 
> — @getpochi

> It’s designed to take away your skills and make you feel great at the same time.
> 
> — @Keepitsimple_27

## Open questions / follow-ups

- How can AI interfaces surface their "execution path" and "reasoning steps" more effectively during the review phase?
- What are the long-term implications of senior developers "losing confidence" in their own ability to keep systemic complexity in their heads?
- Is there a "complexity threshold" beyond which AI-generated code should always be rejected if it cannot be fully mental-modeled?

## Next steps

- Integrate these insights into the [[Maker-Checker Pattern]] note, specifically the "Checker" fatigue aspect.
- Update [[Rebuild Threshold]] to include the psychological "confidence threshold" as a factor in when to discard AI "slop."
- **Contrast with Industry Trends**: Compare the personal "confidence gap" here with the industry-wide "performance gains" discussed in [[distilled/2026-01-08T220243+0200 - When AI writes almost all code, what happens to software engineering?.md]].

## Links

- Source: [https://x.com/hey_yogini/status/2009259026438344790](https://x.com/hey_yogini/status/2009259026438344790)
- Related Industry Insight: [[distilled/2026-01-08T220243+0200 - When AI writes almost all code, what happens to software engineering?.md]]
